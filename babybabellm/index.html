<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BabyBabelLM</title>
    <link rel="icon" href="../images/pacifier.png">
    <link rel="stylesheet" href="https://unpkg.com/leaflet@1.9.4/dist/leaflet.css" 
          integrity="sha256-p4NxAoJBhIIN+hmNHrzRCf9tD/miZyoHS5obTRR9BMY=" 
          crossorigin=""/>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div style="display:inline">
        <img style="float: left; padding-right: 20px;" src="../images/pacifier.png" height="80"> 
        <div class="master-title"> <b>Baby</b>BabelLM </div> 
        <div class="subheader"> A Multilingual Benchmark of Developmentally Plausible Training Data </div> 
    </div>
    <div id="navbar" style="margin-top: 10px;">
        <h4>
            <a href="#" target="_blank"> Paper </a> •
            <a href="https://huggingface.co/collections/BabyLM-community/babybabellm-68be929326975fa8341efed9" target="_blank"> HuggingFace Datasets </a> •
            <a href="https://github.com/babylm-org/multilingual-evaluation/" target="_blank"> Evaluation Repo </a> •
            <a href="https://github.com/babylm-org/multilingual-babylm" target="_blank"> Data Creation Repo </a> •
            <a href="https://github.com/babylm-org/multilingual-training" target="_blank"> Model Training Repo </a> •
            <a href="https://babylm.github.io" target="_blank"> BabyLM </a>
            <hr>
        </h4>
    </div>
    <div class="map-container">
        <div id="map"></div>
    </div>
    <div class="title"> Overview </div> <br>
    <div class="paragraph">
        We present BabyBabelLM, a multilingual collection of datasets modeling the language a person observes from birth until they acquire a native language. We curate developmentally plausible pretraining data aiming to cover the equivalent of 100M English words of content in each of 45 languages. We compile evaluation suites and train baseline models in each language. BabyBabelLM aims to facilitate multilingual pretraining and cognitive modeling.
    </div>
    <div class="title"> Dataset Construction </div> <br>

<ul style="margin-left: 20px;">
        <li><div class="paragraph"> <b> Composition: </b> In constructing BabyBabelLM our guiding principle was <i> developmental plausibility </i> — the idea that pretraining data should approximate the child's linguistic environment. To this end, we prioritized domains such as: child-directed speech, educational materials, children's books, and transcribed conversations. We considered a diverse set of sources, including publicly available datasets, open-access children books and wikis, and local experts in linguistics and language acquisition. This involved both the curation of available developmentally plausible resources, and the addition of original data.  
        <br><br> 
        For most languages, the collection effort was led by a researcher fluent or familiar with that language, responsible for gathering appropriate corpora and ensuring dataset quality. In addition to language-specific data, languages were supplemented with extensive multilingual resources (e.g., <a href="https://talkbank.org/childes/">CHILDES</a>, <a href="https://github.com/cisnlp/GlotStoryBook">GlotStoryBooks</a>), which also comprised the main dataset content for languages without a dedicated leader. Synthetic data were deliberately excluded due to their qualitative differences from naturally occurring language.     </div></li>


        <li><div class="paragraph"> <b> Preprocessing </b> : After language-specific collection, all data were uniformly processed through a shared preprocessing pipeline developed as part of the project, ensuring comparability across languages. In addition, each dataset was annotated with rich document-level metadata in a standardized format, describing the document’s source, content, age estimate, license, and other information.      </div></li>

        
        <li><div class="paragraph"> <b> Language Tiers </b>: Given the difference in availability of developmentally plausible corpora, we sorted languages into three tiers based on training set size, corresponding to the equivalent of respectively 100M, 10M, or 1M English words. Dataset sizes were calibrated by language-adjusted byte estimates (<a href="https://aclanthology.org/2024.sigul-1.1.pdf">paper link</a>), to maintain comparability of data budgets across languages with differing orthographic and morphological characteristics. </div></li>


        <li><div class="paragraph"> <b> Licensing </b>: Finally, throughout collection, we took great care in verifying that all data were released under licenses that permit academic research, and we release our data with document-level licensing information and data source attribution to ensure each resource is used ethically and responsibly. </div></li>



</ul>

<div class="title"> Contributing </div> <br>



<div class="paragraph">
BabyBabelLM is a living resource, created, maintained, and updated by language communities.
We welcome community contributions, in two different forms:
            <br><br> 

<ul style="margin-left: 20px;">

        <li><b>Updating a dataset </b> for a language that is already included in BabyBabelLM, by adding, removing, or updating collected data.</li>
        <br> 

            The project's pipeline <a href="https://github.com/babylm-org/multilingual-babylm">(GitHub)</a> has built-in functionality for processing data in a standardized format, and updating or removing data from a HuggingFace BabyBabelLM repository for a language. To upload the resulting dataset, you can automatically create a <b>pull request</b> on the <a href="https://huggingface.co/collections/BabyLM-community/babybabellm-68be929326975fa8341efed9"> HuggingFace Hub</a> using the pipeline's corresponding commandline arguments. Afterwards, a member of BabyBabelLM will review and incorporate changes into the dataset. For more information please see the project's pipeline on Github. 
            <br><br> 




        <li><b>Creating a dataset </b> by adding data for a language not present in BabyBabelLM.</li>

        <br> 
        Please contact as, so we can first create the corresponding BabyBabelLM dataset for the language, which can then be updated by pull request like above. Contact can be made through opening an issue on the pipeline's <a href="">GitHub</a>.


    </ul>
</div>






    <div class="title"> Baseline Models and Evaluation </div> <br>
    <div class="paragraph">
        We train a set of monolingual and multilingual baseline models and evaluated them on a set of benchmarks.
    </div>
    <div class="title"> Organization Team </div> <br>
    <div class="people">
        <div class="bullet">• Jaap Jumelet (University of Groningen)</div>
        <div class="bullet">• Abdellah Fourtassi (Aix Marseille University )</div>
        <div class="bullet">• Akari Haga (Nara Institute of Science and Technology)</div>
        <div class="bullet">• Bastian Bunzeck (Bielefeld University)</div>
        <div class="bullet">• Bhargav Shandilya (University of Colorado Boulder)</div>
        <div class="bullet">• Diana Galvan-Sosa (University of Cambridge, SomosNLP)</div>
        <div class="bullet">• Faiz Ghifari Haznitrama (KAIST)</div>
        <div class="bullet">• Francesca Padovani  (University of Groningen )</div>
        <div class="bullet">• Francois Meyer (University of Cape Town)</div>
        <div class="bullet">• Hai Hu (City University of Hong Kong)</div>
        <div class="bullet">• Julen Etxaniz (HiTZ, University of the Basque Country)</div>
        <div class="bullet">• Laurent Prévot (Aix Marseille University )</div>
        <div class="bullet">• Linyang He (Columbia University)</div>
        <div class="bullet">• María Grandury (SomosNLP)</div>
        <div class="bullet">• Mila Marcheva (University of Cambridge)</div>
        <div class="bullet">• Negar Foroutan (EPFL)</div>
        <div class="bullet">• Nikitas Theodoropoulos (Independent Researcher)</div>
        <div class="bullet">• Pouya Sadeghi (University of Waterloo)</div>
        <div class="bullet">• Siyuan Song (University of Texas at Austin)</div>
        <div class="bullet">• Suchir Salhan (University of Cambridge)</div>
        <div class="bullet">• Susana Zhou (SomosNLP)</div>
        <div class="bullet">• Yurii Paniv (Ukrainian Catholic University)</div>
        <div class="bullet">• Ziyin Zhang (Shanghai Jiao Tong University)</div>
        <div class="bullet">• Arianna Bisazza (University of Groningen)</div>
        <div class="bullet">• Alex Warstadt (University of California San Diego)</div>
        <div class="bullet">• Leshem Choshen (MIT, MIT-IBM watson AI Lab)</div>
    </div>
    <script src="https://unpkg.com/leaflet@1.9.4/dist/leaflet.js" 
            integrity="sha256-20nQCchB9co0qIjJZRGuk2/Z9VM+kNiyxNV1lvTlZBo=" 
            crossorigin=""></script>
    <script src="script.js"></script>
</body>
</html> 
