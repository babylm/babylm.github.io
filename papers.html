<!DOCTYPE html>
<html>
<head>
  <link rel="stylesheet" href="stylesheet.css">
  <link rel="icon" href="./images/pacifier.png">
</head>

<body>

<div style="display:inline">
  <img style="float: left; padding-right: 20px;" src="./images/pacifier.png" height="80"> 
  <div class="master-title"> <b>Baby</b>LM Challenge </div> 
  <div class="subheader"> Sample-efficient pretraining on a developmentally plausible corpus </div> 
</div>

<div id="navbar">
  <h4> <a href="index.html"> Overview </a> â€¢ <a href="Workshop_times.html"> Workshop Schedule </a> â€¢ <a href="guidelines.html"> Guidelines </a> â€¢ <a href="timeline.html"> Timeline</a> â€¢ <a href="faqs.html"> FAQs </a>â€¢ <a href="papers.html"> Previous papers </a> <hr> </h4> 
</div>

<div class="paragraph"> Here are the previous Calls for Papers and Findings of the BabyLM workshop:</div>

<div class="edition">
  <h2>3rd Edition (Current)</h2>
  ðŸ“„<a href="https://arxiv.org/pdf/2502.10645">Call for Papers</a></li>
    <!-- Add Findings and submissions when available -->
</div>

<div class="edition">
  <h2>2nd Edition</h2>
    ðŸ“„<a href="https://arxiv.org/pdf/2404.06214">Call for Papers</a><br>
<!--     ðŸ“–<a href="https://aclanthology.org/2024.conll-babylm.0.pdf">Proceedings</a><br> -->
    ðŸ“œ<a href="https://arxiv.org/pdf/2412.05149v1">Findings</a><br>
    <b>Submissions:</b>
      <blockquote>
      <ul>
          <li>Ghanizadeh, M. A., & Dousti, M. J. (2024). Towards data-efficient language models: A child-inspired approach to language learning. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Nair, A., Hancharova, A., Kumar, M., & Gharaee, A. (2024). BabyLM challenge: Experimenting with self-distillation and reverse-distillation for language model pre-training on constrained datasets. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Goriely, Z., Diehl Martinez, R., Caines, A., Buttery, P., & Beinborn, L. (2024). From babble to words: Pre-training language models on continuous streams of phonemes. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Bunzeck, B., Duran, D., Schade, L., & ZarrieÃŸ, S. (2024). Graphemes vs. phonemes: Battling it out in character-based language models. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Saha, R., Fahim, A., Fyshe, A., & Murphy, A. (2024). Exploring curriculum learning for vision-language tasks: A study on small-scale multimodal training. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Haller, P., Golde, J., & Akbik, A. (2024). BabyHGRN: Exploring RNNs for sample-efficient language modeling. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Shi, S., Matusevych, Y., & Nissim, M. (2024). Choosy babies need one coach: Inducing mode-seeking behavior in BabyLlama with reverse KL divergence. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Chesi, C., Bressan, V., Barbini, M., Fusco, A., Piccini Bianchessi, M. L., Neri, S., & Rossi, S., Sgrizzi, T. (2024). Different ways to forget: Linguistic gates in recurrent neural networks. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Klerings, A., Bartelt, C., & Mueller, A. (2024). Developmentally plausible multimodal language models are highly modular. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Behr, R. (2024). ELC-ParserBERT: Low-resource language modeling utilizing a parser network with ELC-BERT. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>PrÃ©vot, L., Wang, S.-F., Chi, J.-A., & Hsieh, S.-K. (2024). Extending the BabyLM initiative: Promoting diversity in datasets and metrics through high-quality linguistic corpora. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Berend, G. (2024). Integrating quasi-symbolic conceptual knowledge into language model pre-training. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Edman, L., Bylinina, L., Ghorbanpour, F., & Fraser, A. (2024). Are BabyLMs second language learners? In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Salhan, S., Diehl Martinez, R., Goriely, Z., & Buttery, P. (2024). Less is more: Pre-training cross-lingual small-scale language models with cognitively-plausible curriculum learning strategies. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Capone, L., Bondielli, A., & Lenci, A. (2024). ConcreteGPT: A baby GPT-2 based on lexical concreteness and curriculum learning. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Iyer, S. (2024). When babies teach babies: Can student knowledge sharing outperform teacher-guided distillation on small datasets? In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Nguyen, H., Yip, L., & DeBenedetto, J. (2024). Automatic quality estimation for data selection and curriculum learning. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Lucas, E., Gaines, D., Kosireddy, T. R., Li, K., & Havens, T. C. (2024). Using curriculum masking based on child language development to train a large language model with limited training data. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Lyman, A., & Hepner, B. (2024). WhatIf: Leveraging word vectors for small-scale data augmentation. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Hong, X., LoÃ¡iciga, S., & Sayeed, A. (2024). A surprisal oracle for when every layer counts. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>AlKhamissi, B., Tang, Y., GÃ¶kce, A., Mehrer, J., & Schrimpf, M. (2024). Dreaming out loud: A self-synthesis approach for training vision-language models with developmentally plausible data. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Haga, A., Fukatsu, A., Oba, M., Bisazza, A., & Oseki, Y. (2024). BabyLM challenge: Exploring the effect of variation sets on language model training efficiency. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Charpentier, L. G. G., & Samuel, D. (2024). BERT or GPT: Why not both? In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Yam, H. M., & Paek, N. (2024). What should baby models read? Exploring sample-efficient data composition on model performance. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Tastet, J.-L., & Timiryasov, I. (2024). BabyLlama-2: Ensemble-distilled models consistently outperform teachers with limited data. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Yam, H. M., & Paek, N. (2024). Teaching tiny minds: Exploring methods to enhance knowledge distillation for small language models. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Theodoropoulos, N., Filandrianos, G., Lyberatos, V., Lymperaiou, M., & Stamou, G. (2024). BERTtime stories: Investigating the role of synthetic story data in language pre-training. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
          <li>Yu, X., Guo, B., Luo, S., Wang, J., Ji, T., & Wu, Y. (2024). AntLM: Bridging causal and masked language models. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
      </ul>
       </blockquote>

</div>

<div class="edition">
  <h2>1st Edition</h2>
  
    ðŸ“„<a href="https://arxiv.org/pdf/2301.11796">Call for Papers</a><br>
    ðŸ“–<a href="https://aclanthology.org/2023.conll-babylm.0.pdf">Proceedings</a><br>
    ðŸ“œ<a href="https://aclanthology.org/2023.conll-babylm.1.pdf">Findings</a><br>
    <b>Submissions:</b>
      <blockquote>
      <ul>
        <li>Bastian Bunzeck, & Sina ZarrieÃŸ (2023). GPT-wee: How Small Can a Small Language Model Really Get?. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.2.pdf</li>
        <li>Clayton Fields, Osama Natouf, Andrew McMains, Catherine Henry, & Casey Kennington (2023). Tiny Language Models Enriched with Multimodal Knowledge from Multiplex Networks. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.3.pdf</li>
        <li>Irina Proskurina, Guillaume Metzler, & Julien Velcin (2023). Mini Minds: Exploring Bebeshka and Zlata Baby Models. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.4.pdf</li>
        <li>Xuanda Chen, & Eva Portelance (2023). Grammar induction pretraining for language modeling in low resource contexts. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.5.pdf</li>
        <li>Jaap Jumelet, Michael Hanna, Marianne de Heer Kloots, Anna Langedijk, Charlotte Pouw, & Oskar van der Wal (2023). ChapGTP,ILLCâ€™s Attempt at Raising aBabyLM: Improving Data Efficiency by Automatic Task Formation. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.6.pdf</li>
        <li>Yahan Yang, Elior Sulem, Insup Lee, & Dan Roth (2023). Penn &BGUBabyBERTa+ for Strict-SmallBabyLMChallenge. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.7.pdf</li>
        <li>Lukas Edman, & Lisa Bylinina (2023). Too Much Information: Keeping Training Simple forBabyLMs. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.8.pdf</li>
        <li>Aryaman Chobey, Oliver Smith, Anzi Wang, & Grusha Prasad (2023). Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.9.pdf</li>
        <li>Richard Diehl Martinez, ZÃ©bulon Goriely, Hope McGovern, Christopher Davis, Andrew Caines, Paula Buttery, & Lisa Beinborn (2023). CLIMBâ€“ Curriculum Learning for Infant-inspired Model Building. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.10.pdf</li>
        <li>Theodor Amariucai, & Alexander Scott Warstadt (2023). Acquiring Linguistic Knowledge from Multimodal Input. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.11.pdf</li>
        <li>Julius Steuer, Marius Mosbach, & Dietrich Klakow (2023). LargeGPT-like Models are Bad Babies: A Closer Look at the Relationship between Linguistic Competence and Psycholinguistic Measures. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.12.pdf</li>
        <li>Zheyu Zhang, Han Yang, Bolei Ma, David RÃ¼gamer, & Ercong Nie (2023). Babyâ€™sCoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.13.pdf</li>
        <li>Ã–mer Veysel Ã‡aÄŸatan (2023). ToddlerBERTa: ExploitingBabyBERTa for Grammar Learning and Language Understanding. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.14.pdf</li>
        <li>Lukas Thoma, Ivonne Weyers, Erion Ã‡ano, Stefan Schweter, Jutta L Mueller, & Benjamin Roth (2023). CogMemLM: Human-Like Memory Mechanisms Improve Performance and Cognitive Plausibility ofLLMs. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.15.pdf</li>
        <li>Xingmeng Zhao, Tongnian Wang, Sheri Osborn, & Anthony Rios (2023). BabyStories: Can Reinforcement Learning Teach Baby Language Models to Write Better Stories?. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.16.pdf</li>
        <li>Justin DeBenedetto (2023). Byte-ranked Curriculum Learning forBabyLMStrict-small Shared Task 2023. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.17.pdf</li>
        <li>Ziling Cheng, Rahul Aralikatte, Ian Porada, Cesare Spinoso-Di Piano, & Jackie CK Cheung (2023). McGillBabyLMShared Task Submission: The Effects of Data Formatting and Structural Biases. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.18.pdf</li>
        <li>David Samuel (2023). MeanBERTs make erratic language teachers: the effectiveness of latent bootstrapping in low-resource settings. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.19.pdf</li>
        <li>Lucas Georges Gabriel Charpentier, & David Samuel (2023). Not all layers are equally as important: Every Layer CountsBERT. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.20.pdf</li>
        <li>Lukas Wolf, Klemen Kotar, Greta Tuckute, Eghbal Hosseini, Tamar I. Regev, Ethan Gotlieb Wilcox, & Alexander Scott Warstadt (2023). WhisBERT: Multimodal Text-Audio Language Modeling on 100MWords. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.21.pdf</li>
        <li>Xudong Hong, Sharid LoÃ¡iciga, & Asad Sayeed (2023). A surprisal oracle for active curriculum language modeling. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.22.pdf</li>
        <li>Maggie Mi (2023). Mmi01 at TheBabyLMChallenge: Linguistically Motivated Curriculum Learning for Pretraining in Low-Resource Settings. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.23.pdf</li>
        <li>Inar Timiryasov, & Jean-Loup Tastet (2023). Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.24.pdf</li>
        <li>Miyu Oba, Akari Haga, Akiyo Fukatsu, & Yohei Oseki (2023). BabyLMChallenge: Curriculum learning based on sentence complexity approximating language acquisition. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.25.pdf</li>
        <li>GÃ¡bor Berend (2023). Better Together: Jointly Using Masked Latent Semantic Modeling and Masked Language Modeling for Sample Efficient Pre-training. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.26.pdf</li>
        <li>Venkata S Govindarajan, Juan Diego Rodriguez, Kaj Bostrom, & Kyle Mahowald (2023). Lil-Bevo: Explorations of Strategies for Training Language Models in More Humanlike Ways. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.27.pdf</li>
        <li>Chenghao Xiao, G Thomas Hudson, & Noura Al Moubayed (2023). Towards more Human-like Language Models based on Contextualizer Pretraining Strategy. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.28.pdf</li>
        <li>Omar Momen, David Arps, & Laura Kallmeyer (2023). Increasing The Performance of Cognitively Inspired Data-Efficient Language Models via Implicit Structure Building. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.29.pdf</li>
        <li>Khushi Bhardwaj, Raj Sanjay Shah, & Sashank Varma (2023). Pre-trainingLLMs using human-like development data corpus. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.30.pdf</li>
        <li>Mattia Opper, J. Morrison, & N. Siddharth (2023). On the effect of curriculum learning with developmental data for grammar acquisition. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.31.pdf</li>
        <li>Nasim Borazjanizadeh (2023). OptimizingGPT-2 Pretraining onBabyLMCorpus with Difficulty-based Sentence Reordering. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.32.pdf</li>
      </blockquote>
      </ul>

</div>

<div class="footer">
  <div style="float:right;"> Images provided by Smashicons </div>
</div>

</body>
</html>
