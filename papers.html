<!DOCTYPE html>
<html>
<head>
  <link rel="stylesheet" href="stylesheet.css">
  <link rel="icon" href="./images/pacifier.png">
</head>

<body>

<div style="display:inline">
  <img style="float: left; padding-right: 20px;" src="./images/pacifier.png" height="80"> 
  <div class="master-title"> <b>Baby</b>LM Challenge </div> 
  <div class="subheader"> Sample-efficient pretraining on a developmentally plausible corpus </div> 
</div>

<div id="navbar">
  <h4> <a href="index.html"> Overview </a> • <a href="guidelines.html"> Guidelines </a> • <a href="timeline.html"> Timeline</a> • <a href="faqs.html"> FAQs </a>• <a href="papers.html"> Previous papers </a> <hr> </h4> 
</div>

<div class="paragraph"> Here are the previous Calls for Papers and Findings of the BabyLM workshop:</div>

<div class="edition">
  <h2>3rd Edition (Current)</h2>
  <ul>
    <li><a href="https://arxiv.org/pdf/2502.10645">Call for Papers</a></li>
    <!-- Add Findings and submissions when available -->
  </ul>
</div>

<div class="edition">
  <h2>2nd Edition</h2>
  <ul>
    <li><a href="https://arxiv.org/pdf/2404.06214">Call for Papers</a></li>
    <li><a href="https://aclanthology.org/2024.conll-babylm.0.pdf">Proceedings</a></li>
    <li><a href="https://arxiv.org/pdf/2412.05149v1">Findings</a></li>
    <li>Submissions:
      <ul>
      <li>Mohammad Amin Ghanizadeh, & Mohammad Javad Dousti (2024). Towards Data-Efficient Language Models: A Child-Inspired Approach to Language Learning. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
      <li>Aakarsh Nair, Alina Hancharova, Mayank Kumar, & Ali Gharaee (2024). BabyLM Challenge: Experimenting with Self-Distillation and Reverse-Distillation for Language Model Pre-Training on Constrained Datasets. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
      <li>Zébulon Goriely, Richard Diehl Martinez, Andrew Caines, Paula Buttery, & Lisa Beinborn (2024). From Babble to Words: Pre-Training Language Models on Continuous Streams of Phonemes. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
      <li>Bastian Bunzeck, Daniel Duran, Leonie Schade, & Sina Zarrieß (2024). Graphemes vs. phonemes: battling it out in character-based language models. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
      <li>Rohan Saha, Abrar Fahim, Alona Fyshe, & Alex Murphy (2024). Exploring Curriculum Learning for Vision-Language Tasks: A Study on Small-Scale Multimodal Training. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
      <li>Patrick Haller, Jonas Golde, & Alan Akbik (2024). BabyHGRN: Exploring RNNs for Sample-Efficient Language Modeling. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
      <li>Shaozhen Shi, Yevgen Matusevych, & Malvina Nissim (2024). Choosy Babies Need One Coach: Inducing Mode-Seeking Behavior in BabyLlama with Reverse KL Divergence. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
      <li>Cristiano Chesi, Veronica Bressan, Matilde Barbini, Achille Fusco, Maria Letizia Piccini Bianchessi, Sofia Neri, Sarah Rossi, & Tommaso Sgrizzi (2024). Different Ways to Forget: Linguistic Gates in Recurrent Neural Networks. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
      <li>Alina Klerings, Christian Bartelt, & Aaron Mueller (2024). Developmentally Plausible Multimodal Language Models Are Highly Modular. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
      <li>Rufus Behr (2024). ELC-ParserBERT: Low-Resource Language Modeling Utilizing a Parser Network With ELC-BERT. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
      <li>Laurent Prévot, Sheng-Fu Wang, Jou-An Chi, & Shu-Kai Hsieh (2024). Extending the BabyLM Initiative: Promoting Diversity in Datasets and Metrics through High-Quality Linguistic Corpora. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
      <li>Gábor Berend (2024). Integrating Quasi-symbolic Conceptual Knowledge into Language Model Pre-training. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
      <li>Lukas Edman, Lisa Bylinina, Faeze Ghorbanpour, & Alexander Fraser (2024). Are BabyLMs Second Language Learners?. In <i>The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning</i>.</li>
    </ul>
    </li>
  </ul>
</div>

<div class="edition">
  <h2>1st Edition</h2>
  <ul>
    <li><a href="https://arxiv.org/pdf/2301.11796">Call for Papers</a></li>
    <li><a href="https://aclanthology.org/2023.conll-babylm.0.pdf">Proceedings</a></li>
    <li><a href="https://aclanthology.org/2023.conll-babylm.1.pdf">Findings</a></li>
    <li>Submissions:
      <ul>
        <li>Bastian Bunzeck, & Sina Zarrieß (2023). GPT-wee: How Small Can a Small Language Model Really Get?. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.2.pdf</li>
        <li>Clayton Fields, Osama Natouf, Andrew McMains, Catherine Henry, & Casey Kennington (2023). Tiny Language Models Enriched with Multimodal Knowledge from Multiplex Networks. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.3.pdf</li>
        <li>Irina Proskurina, Guillaume Metzler, & Julien Velcin (2023). Mini Minds: Exploring Bebeshka and Zlata Baby Models. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.4.pdf</li>
        <li>Xuanda Chen, & Eva Portelance (2023). Grammar induction pretraining for language modeling in low resource contexts. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.5.pdf</li>
        <li>Jaap Jumelet, Michael Hanna, Marianne de Heer Kloots, Anna Langedijk, Charlotte Pouw, & Oskar van der Wal (2023). ChapGTP,ILLC’s Attempt at Raising aBabyLM: Improving Data Efficiency by Automatic Task Formation. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.6.pdf</li>
        <li>Yahan Yang, Elior Sulem, Insup Lee, & Dan Roth (2023). Penn &BGUBabyBERTa+ for Strict-SmallBabyLMChallenge. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.7.pdf</li>
        <li>Lukas Edman, & Lisa Bylinina (2023). Too Much Information: Keeping Training Simple forBabyLMs. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.8.pdf</li>
        <li>Aryaman Chobey, Oliver Smith, Anzi Wang, & Grusha Prasad (2023). Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.9.pdf</li>
        <li>Richard Diehl Martinez, Zébulon Goriely, Hope McGovern, Christopher Davis, Andrew Caines, Paula Buttery, & Lisa Beinborn (2023). CLIMB– Curriculum Learning for Infant-inspired Model Building. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.10.pdf</li>
        <li>Theodor Amariucai, & Alexander Scott Warstadt (2023). Acquiring Linguistic Knowledge from Multimodal Input. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.11.pdf</li>
        <li>Julius Steuer, Marius Mosbach, & Dietrich Klakow (2023). LargeGPT-like Models are Bad Babies: A Closer Look at the Relationship between Linguistic Competence and Psycholinguistic Measures. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.12.pdf</li>
        <li>Zheyu Zhang, Han Yang, Bolei Ma, David Rügamer, & Ercong Nie (2023). Baby’sCoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.13.pdf</li>
        <li>Ömer Veysel Çağatan (2023). ToddlerBERTa: ExploitingBabyBERTa for Grammar Learning and Language Understanding. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.14.pdf</li>
        <li>Lukas Thoma, Ivonne Weyers, Erion Çano, Stefan Schweter, Jutta L Mueller, & Benjamin Roth (2023). CogMemLM: Human-Like Memory Mechanisms Improve Performance and Cognitive Plausibility ofLLMs. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.15.pdf</li>
        <li>Xingmeng Zhao, Tongnian Wang, Sheri Osborn, & Anthony Rios (2023). BabyStories: Can Reinforcement Learning Teach Baby Language Models to Write Better Stories?. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.16.pdf</li>
        <li>Justin DeBenedetto (2023). Byte-ranked Curriculum Learning forBabyLMStrict-small Shared Task 2023. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.17.pdf</li>
        <li>Ziling Cheng, Rahul Aralikatte, Ian Porada, Cesare Spinoso-Di Piano, & Jackie CK Cheung (2023). McGillBabyLMShared Task Submission: The Effects of Data Formatting and Structural Biases. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.18.pdf</li>
        <li>David Samuel (2023). MeanBERTs make erratic language teachers: the effectiveness of latent bootstrapping in low-resource settings. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.19.pdf</li>
        <li>Lucas Georges Gabriel Charpentier, & David Samuel (2023). Not all layers are equally as important: Every Layer CountsBERT. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.20.pdf</li>
        <li>Lukas Wolf, Klemen Kotar, Greta Tuckute, Eghbal Hosseini, Tamar I. Regev, Ethan Gotlieb Wilcox, & Alexander Scott Warstadt (2023). WhisBERT: Multimodal Text-Audio Language Modeling on 100MWords. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.21.pdf</li>
        <li>Xudong Hong, Sharid Loáiciga, & Asad Sayeed (2023). A surprisal oracle for active curriculum language modeling. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.22.pdf</li>
        <li>Maggie Mi (2023). Mmi01 at TheBabyLMChallenge: Linguistically Motivated Curriculum Learning for Pretraining in Low-Resource Settings. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.23.pdf</li>
        <li>Inar Timiryasov, & Jean-Loup Tastet (2023). Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.24.pdf</li>
        <li>Miyu Oba, Akari Haga, Akiyo Fukatsu, & Yohei Oseki (2023). BabyLMChallenge: Curriculum learning based on sentence complexity approximating language acquisition. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.25.pdf</li>
        <li>Gábor Berend (2023). Better Together: Jointly Using Masked Latent Semantic Modeling and Masked Language Modeling for Sample Efficient Pre-training. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.26.pdf</li>
        <li>Venkata S Govindarajan, Juan Diego Rodriguez, Kaj Bostrom, & Kyle Mahowald (2023). Lil-Bevo: Explorations of Strategies for Training Language Models in More Humanlike Ways. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.27.pdf</li>
        <li>Chenghao Xiao, G Thomas Hudson, & Noura Al Moubayed (2023). Towards more Human-like Language Models based on Contextualizer Pretraining Strategy. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.28.pdf</li>
        <li>Omar Momen, David Arps, & Laura Kallmeyer (2023). Increasing The Performance of Cognitively Inspired Data-Efficient Language Models via Implicit Structure Building. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.29.pdf</li>
        <li>Khushi Bhardwaj, Raj Sanjay Shah, & Sashank Varma (2023). Pre-trainingLLMs using human-like development data corpus. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.30.pdf</li>
        <li>Mattia Opper, J. Morrison, & N. Siddharth (2023). On the effect of curriculum learning with developmental data for grammar acquisition. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.31.pdf</li>
        <li>Nasim Borazjanizadeh (2023). OptimizingGPT-2 Pretraining onBabyLMCorpus with Difficulty-based Sentence Reordering. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Retrieved from https://aclanthology.org/2023.conll-babylm.32.pdf</li>

      </ul>
    </li>
  </ul>
</div>

<div class="footer">
  <div style="float:right;"> Images provided by Smashicons </div>
</div>

</body>
</html>
