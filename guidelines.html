<!DOCTYPE html>
<html>
<head>
  <link rel="stylesheet" href="stylesheet.css">
  <link rel="icon" href="./images/pacifier.png">
</head>

<body>

<div style="display:inline">
 <img style="float: left; padding-right: 20px;" src="./images/pacifier.png" height="80"> 
 <div class="master-title"> <b>Baby</b>LM Challenge </div> 
 <div class="subheader"> Sample-efficient pretraining on a developmentally plausible corpus </div> 
</div>

<div id="navbar">
<h4> <a href="index.html"> Overview </a> • <a href="guidelines.html"> Guidelines </a> • <a href="timeline.html"> Timeline</a> • <a href="faqs.html"> FAQs </a>• <a href="papers.html"> Previous papers </a> <hr> </h4> 
</div>

<div class="paragraph"> Submissions should be implemented in Huggingface's Transformers library. Participants can choose whatever model architecture they wish, as long as submissions can assign log-likelihoods or pseudo log-likelihoods to strings of text.</div>


<div class="title"> Submission Tracks </div>

<div class="paragraph"> There are three competition categories: <b> Interaction </b>, which is new for this year's competition; <b> multimodal </b>, <b> strict </b>, and <b> strict-small </b>. We will additionally accept papers in a dedicated <b> paper track </b> </div>

<div class="bullet"> <b> • Strict </b> and <b> Strict-Small Tracks: </b> The strict and strict-small tracks require that submissions are trained on 100M words (for strict) or 10M words (for strict small) of text data. Unlike last year, participants can construct their own pretraining datasets, as long as they are under the 100M or 10M word budget. To help you get started, we release 100M and 10M word pretraining datasets, which are largely similar to the ones developed for last year's competition. Submissions in these tracks will be evaluated on language-only evaluation tasks. </div>

<div class="bullet"> <b> • Multimodal Track: </b> Submissions must be trained on 100M words or less, however, they can be trained on any amount of non-linguistic data. Submissions will be evaluated on language-only and language-vision tasks, meaning that successful entrants will likely pre-train on text-image data. To help you get started, we are releasing a 50M word text-only and 50M word paired text-image dataset. </div>

  <div class="bullet"> <b> • Interaction Track: </b> The Interaction track debuts this year to allow for interaction between multiple agents during training. We will distinguish between a <b> submission model</b>, i.e., the participants' entry into the competition, and an <b>external model</b>, i.e., a secondary model used in the training pipeline of the submission model but not submitted to the competition. External models must come from a predetermined list of models available on the BabyLM website. External models may be fine-tuned or distilled without restriction. However, the submission model must be exposed to no more than 100M word tokens (multiple exposures allowed, e.g., epochs); this word count includes text generated by external models and pre-existing corpora. Additionally, the submission model may not generate more than 100M words during the training process. Finally, the external model's weights, hidden states, or output distribution cannot be revealed to the submission model. </div>

<div class="bullet"> <b> • Paper Track: </b> In this track, we will accept any contribution related to human-scale or data-efficient language modeling, or cognitive modeling using language models. While contributions can describe language modeling architectures, we also welcome new datasets and evaluation tasks. </div>


<div class="title"> Pretraining Data </div>

<div class="paragraph"> [<a href="https://osf.io/ad7qg/"> Click here to access data (via OSF) </a>] To help you get started, we distribute pretraining datasets for the strict, strict-small and multimodal track. Note, however that, unlike last year, participants are free to construct their own datasets if they wish. Below, we give a few more details on the datasets we provide: </div>

<div class="bullet"><b> • Text-only Dataset: </b> Our text-only dataset is an updated version of last year's BabyLM training corpus. It comes in 10M and 100M word variants, consists mostly of transcribed speech, and has a large proportion of simplified language, such as child-directed speech, childrens' storybooks, and simple Wikipedia. </div>

<div class="bullet"><b> • Multimodal Dataset: </b> Our multimodal dataset consists of a 50M word down-sampled version of our text-only dataset, and 50M words of paired image-text data. </div>

<div class="paragraph"> See the <a href="https://babylm.github.io/2024-cfp.pdf">updated call for papers</a> for a detailed breakdown of the pretraining datasets.</div>


<div class="title"> Evaluation Pipeline </div>

<div class="paragraph"> Models will be evaluated on a shared pipeline, which will be released on GitHub in late April. The evaluation pipeline will come in two variants: a text-only evaluation, which is required for strict and strict-small track participants, and a vision-language variant required for multimodal track participants. </div>


<div class="title"> Results Submissions </div>
The details for the submission of the results and the paper will be shared soon. In the meanwhile, checkout the timeline for tentative dates.
<!--   <div class="paragraph"> The deadline for results submissions is <strong> September 16, 23:59 anywhere on earth (UTC-12)</strong>.</div>
  <div class="paragraph"> Submissions must be made through <a href="https://openreview.net/group?id=EMNLP/2024/Workshop/CoNLL_Shared_Task/BabyLM_Challenge"> OpenRevew </a>. To fill out the submission, please prepare these two things:
A HuggingFace link to your models.
A download link to your results, assembled via the `collect_results.py` script in <a href="https://github.com/babylm/evaluation-pipeline-2024"> babylm/evaluation-pipeline-2024 </a>.  </div> -->


<div class="title"> Paper Submissions </div>
  
<div class="paragraph"> Along with their model submissions, everyone must submit a paper. This can be a short technical description of the proposed approach or a longer contribution, up to 8 pages.</div>

<div class="paragraph"> Submissions will be made through our OpenRevew portal. Note that hyperparameters and decisions should be stated in the paper but also filled in a <a href="https://forms.gle/nRjdt5w5rCoFFqnJ6"> form </a> to assure same format and ease of future use </div>
  
  <div class="paragraph"> Submissions of both types are:</div>

  <div class="bullet"><b>•</b> given unlimited space for references,</div>
  <div class="bullet"><b>•</b> given unlimited space for appendices,</div>
  <div class="bullet"><b>•</b> given extra space for ethics/limitations, though these sections are optional</div>

  <div class="paragraph">We allow <b>dual submissions</b> of archival papers. If an archival paper is accepted by both BabyLM and another venue, it can only appear in one of their proceedings (i.e., it must be withdrawn from one venue). </div>

  <div class="paragraph">BabyLM will hold its own <b>review process</b>, and the proceedings will appear in their own volume. The acceptance criteria are based on soundness and fit: We plan only to reject submissions that make incorrect or unjustified claims or else are not related to the BabyLM topic. Other feedback will be directed toward improving submissions.</div>

<div class="title"> Outstanding Paper Awards </div>

<div class="paragraph">In addition to track winners, we will also award several "outstanding paper" awards. We intend to give these awards to submissions that are innovative or unusual or make novel and significant connections between language modeling and psycholinguistics research topics. </div>




<div class="footer">
<div style="float:right;"> Images provided by Smashicons </div>
</div>

</body>
