<!DOCTYPE html>
<html>
<head>
  <link rel="stylesheet" href="stylesheet.css">
  <link rel="icon" href="./images/pacifier.png">
</head>

<body>

<div style="display:inline">
 <img style="float: left; padding-right: 20px;" src="./images/pacifier.png" height="80"> 
 <div class="master-title"> <b>Baby</b>LM Challenge </div> 
 <div class="subheader"> Sample-efficient pretraining on a developmentally plausible corpus </div> 
</div>

<div id="navbar">
<h4> <a href="index.html"> Overview </a> • <a href="guidelines.html"> Guidelines </a> • <a href="timeline.html"> Timeline</a> • <a href="faqs.html"> FAQs </a> <hr> </h4> 
</div>

<div class="paragraph"> Submissions should be implemented in Huggingface's Transformers library. Participants can choose whatever model architecture they wish, as long as submissions can assign log-likelihoods or pseudo log-likelihoods to strings of text.</div>


<div class="title"> Submission Tracks </div>

<div class="paragraph"> There are three competition categories: <b> multimodal </b>, which is new for this year's competition, <b> strict </b>, and <b> strict-small </b>. We will additionally accept papers in a dedicated <b> paper track </b> </div>

<div class="bullet"> <b> • Strict </b> and <b> Strict-Small Tracks: </b> The strict and strict-small tracks require that submissions are trained on 100M words (for strict) or 10M words (for strict small) of text data. Unlike last year, participants can construct their own pretraining datasets, as long as they are under the 100M or 10M word budget. To help you get started, we release 100M and 10M word pretraining datasets, which are largely similar to the ones developed for last year's competition. Submissions in these tracks will be evaluated on language-only evaluation tasks. </div>

<div class="bullet"> <b> • Multimodal Track: </b> Submissions must be trained on 100M words or less, however they can be trained on any amount of non-linguistic data. Submissions will be evaluated on language-only and language-vision tasks, meaning that successful entrants will likely pretrain on text-image data. To help you get started, we are releasing a 50M word text-only and 50M word paired text-image dataset. </div>

<div class="bullet"> <b> • Paper Track: </b> In this track, we will accept any contribution related to human-scale or data-efficient language modeling, or cognitive modeling using language models. While contributions can describe language modeling architectures, we also welcome new datasets and evaluation tasks. </div>


<div class="title"> Pretraining Data </div>

<div class="paragraph"> [<a href="https://osf.io/ad7qg/"> Click here to access data (via OSF) </a>] To help you get started, we distribute pretraining datasets for the strict, strict-small and multimodal track. Note, however that, unlike last year, participants are free to construct their own datasets if they wish. Below, we give a few more details on the datasets we provide: </div>

<div class="bullet"><b> • Text-only Dataset: </b> Our text-only dataset is an updated version of last year's BabyLM training corpus. It comes in 10M and 100M word variants, consists mostly of transcribed speech, and has a large proportion of simplified language, such as child-directed speech, childrens' storybooks, and simple Wikipedia. </div>

<div class="bullet"><b> • Multimodal Dataset: </b> Our multimodal dataset consists of a 50M word down-sampled version of our text-only dataset, and 50M words of paired image-text data. </div>

<div class="paragraph"> See the <a href="https://babylm.github.io/2024-cfp.pdf">updated call for papers</a> for a detailed breakdown of the pretraining datasets.</div>


<div class="title"> Evaluation Pipeline </div>

<div class="paragraph"> Models will be evaluated on a shared pipeline, which will be released on Github in late April. The evaluation pipeline will come in two variants, a text-only evaluation, which is required for strict and strict-small track participants, and a vision-language varient that is required for multimodal track participants. </div>


<div class="title"> Results Submissions </div>
  
  <div class="paragraph"> The deadline for results submissions is <strong> September 16, 23:59 anywhere on earth (UTC-12)</strong>.</div>
  <div class="paragraph"> Submissions must be made through <a href="https://openreview.net/group?id=EMNLP/2024/Workshop/CoNLL_Shared_Task/BabyLM_Challenge"> OpenRevew </a>. To fill out the submission, please prepare these two things:
A HuggingFace link to your models.
A download link to your results, assembled via the `collect_results.py` script in <a href="https://github.com/babylm/evaluation-pipeline-2024"> babylm/evaluation-pipeline-2024 </a>.  </div>


<div class="title"> Paper Submissions </div>
  
<div class="paragraph"> Along with their model submissions, everyone must submit a paper. This can be a short technical description of the proposed approach, or a longer contribution, up to 8 pages. The deadline for paper submissions is <strong> September 20, 23:59 anywhere on earth (UTC-12)</strong>.</div>

<div class="paragraph"> Submissions will be made through our <a href="https://openreview.net/group?id=EMNLP/2024/Workshop/CoNLL_Shared_Task/BabyLM_Challenge"> OpenRevew </a> portal. Note that hyperparameters and decisions should be stated in the paper but also filled in a <a href="https://forms.gle/nRjdt5w5rCoFFqnJ6"> form </a> to assure same format and ease of future use </div>
  
  <div class="paragraph"> Submissions of both types are:</div>

  <div class="bullet"><b>•</b> given unlimited space for references,</div>
  <div class="bullet"><b>•</b> given unlimited space for appendices,</div>
  <div class="bullet"><b>•</b> given extra space for ethics/limitations, though these sections are optional</div>

  <div class="paragraph">We allow <b>dual submissions</b> of archival papers. In the event that an archival paper is accepted to both BabyLM and another venue, it can only appear in one of their proceedings (i.e., it must be withdrawn from one venue). </div>

  <div class="paragraph">BabyLM will hold its own <b>review process</b>, and the proceedings will appear in their own volume. The acceptance criteria are based on soundness and fit: We plan only to reject submissions that make incorrect or unjustified claims, or else are not related to the BabyLM topic. Other feedback will be directed at the improvement of submissions.</div>

<div class="title"> Outstanding Paper Awards </div>

<div class="paragraph">In addition to track winners, we will also award several "outstanding paper" awards. We intend to give these awards to submissions that are innovative or unusual, or make novel and significant connections between language modeling and psycholinguistics research topics. </div>




<div class="footer">
<div style="float:right;"> Images provided by Smashicons </div>
</div>

</body>
